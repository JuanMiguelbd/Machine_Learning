
# Lab assignment: SVMs in scikit-learn

In this assignment we will learn how to work with SVMs using the scikit-learn library. We will study in detail their cross-validation, pipelining, training times and kernel functions.

## Guidelines

Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</td></tr>
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>This is a hint or useful observation that can help you solve this assignment. You should pay attention to these hints to better understand the assignment.</td></tr>
 <tr><td width="80"><img src="img/pro.png" style="width:auto;height:auto"></td><td>This is an advanced and voluntary exercise that can help you gain a deeper knowledge into the topic. Good luck!</td></tr>
</table>


During the assignment you will make use of several Python packages that might not be installed in your machine. If that is the case, you can install new Python packages with

    conda install PACKAGENAME
    
if you are using Python Anaconda. Else you should use

    pip install PACKAGENAME

You will need the following packages for this particular assignment. Make sure they are available before proceeding:

* **numpy**
* **matplotlib**
* **scikit-learn**

The following code will embed any plots into the notebook instead of generating a new window:


```python
import matplotlib.pyplot as plt
%matplotlib inline
```

Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells.

Let's go!

## Synthetic dataset

For the first exercises of this assignment we will use the synthetic dataset generated by the following code:


```python
import numpy as np
from sklearn.datasets import make_gaussian_quantiles
RND_STATE=np.random.RandomState(42)
# Build first cluster
X1, y1 = make_gaussian_quantiles(cov=2., n_samples=200, n_features=2, n_classes=2, random_state=RND_STATE)
# Build second cluster
X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=RND_STATE)
# Fuse them, scaling features differently and switching labels of y2
X = np.concatenate((X1, X2))
X[:, 0] *= 10
X[:, 1] /= 10
y = np.concatenate((y1, -y2+1))
```

The dataset consists of 500 points and consists of two Gaussian clusters. In each cluster points in its inner circle belong to one class, and those in the outer circle belong to the other class. Classes are switched in the second cluster so that it is more challenging to discriminate between both classes.

If we plot these data we obtain the following:


```python
from matplotlib.colors import ListedColormap
cm = ListedColormap(['#0000FF', '#FF0000'])    # blue, red
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm)     
plt.show()
```


![png](output_10_0.png)


Once we have generated the data *(X, y)*, let us split them into a **training set** and a **test set**. There is a utility function in scikit-learn that does exactly this:


```python
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=RND_STATE)
```

Since *test_size=0.5*, both the training set and the test set will have 250 points each. The number of features is 2, as specified when invoking *make_gaussian_quantiles*:


```python
print(X_train.shape)
print(X_test.shape)
```

    (250, 2)
    (250, 2)
    

## Scikit-learn basics

Scikit-learn's models for **supervised learning** (SVMs are just one kind of such models) implement a common interface. The most important functions of this interface are the following:

* **fit(X, y)**: trains the model, fitting it for input patterns *X* and outputs *y*.
* **score(X, y)**: tests an already fitted model with additional data, returning the accuracy obtained (i.e., how similar the outputs given by the model are, compared to the true outputs *y*).

Thus, all classifiers and regressors in scikit-learn have specific implementations for the above functions. The differences among models (e.g., the different parameters they use) are treated either during construction or internally within these functions.

For the particular case of SVMs for classification, the implementing class is <a href=http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html>SVC</a>:


```python
from sklearn.svm import SVC
SVC()
```




    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)



As you can see, by default scikit-learn builds an SVM with the **RBF kernel**. We know that such an SVM needs 2 parameters: **C** (regularizer between model complexity and classification errors) and **gamma** (kernel width). These are set to 1 and to 'auto' respectively, where 'auto' stands for 1/d (that is, 1 divided by the number of features).

If you want to create an instance of an SVM with other values for these parameters, you just specify them (the rest of options for the constructor can be safely ignored for now):


```python
SVC(C=5, gamma=0.01)
```




    SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)



In practice it is difficult to know which values for *C* and *gamma* work best for a particular dataset. This is why it is common to try with a **grid** of values for both parameters, selecting the pair of values that result in the highest accuracy.

The first step is selecting the parameter **ranges**. For this exercise, let us use these:


```python
Cs = np.logspace(-2, 4, 7)
gammas = np.logspace(-4, 4, 9)
```

That is, *C* ranges from 0.01 to 10000, whereas *gamma* goes from 0.0001 to 10000, using powers of 10 as intermediate values.

Recall that in order to avoid **overfitting**, we cannot use the test set for **tuning** these parameters. The following code keeps track of the accuracies obtained for the training set, selecting the best *C* and *gamma*:


```python
# Accuracies
accs = np.zeros((len(Cs), len(gammas)))   # all accuracies (in matrix form)
best_acc = 0.0   # best accuracy
# For each C
for i, C in enumerate(Cs):
    # For each sigma
    for j, gamma in enumerate(gammas):
        # Create and train SVM
        svm = SVC(C=C, gamma=gamma).fit(X_train, y_train)
        # Keep track of accuracies and best params
        acc = svm.score(X_train, y_train)
        accs[i, j] = acc
        if acc > best_acc:
            best_C = C
            best_gamma = gamma
            best_acc = acc
```

It is illustrative to plot these accuracies in matrix form:


```python
plt.imshow(accs, interpolation='nearest', cmap=plt.cm.hot)
plt.xlabel('gamma')
plt.ylabel('C')
plt.colorbar()
plt.xticks(np.arange(len(gammas)), gammas, rotation=45)
plt.yticks(np.arange(len(Cs)), Cs)
plt.title('Accuracies obtained')
plt.show()
```


![png](output_25_0.png)


The best parameters turn out to be:


```python
print("Best C = " + str(best_C))
print("Best gamma = " + str(best_gamma))
```

    Best C = 1.0
    Best gamma = 100.0
    

Which give a perfect accuracy on the training set:


```python
print("Best accuracy on train = " + str(best_acc))
```

    Best accuracy on train = 1.0
    

## Cross-validation

Perfect accuracy may seem like great news. However, the performance on the test set is not so good:


```python
best_model = SVC(C=best_C, gamma=best_gamma).fit(X_train, y_train)
best_model.score(X_test, y_test)
```




    0.628



This is a clear sign of overfitting on the training set. If you remember from the theory, ideally we should tune our model fitting it with the training set, but assessing performance on a separate set called **validation set**.

However, usually there are not enough data to split them into separate training, validation and test sets. If that is the case, we can resort to **cross-validation**, which proceeds as follows:
* The training set is partitioned into *k* subsets (called **folds**).
* *k* different models are trained, using each of the *k* folds to assess performance and the remaining *k-1* folds to fit each model.
* The best model is that whose average performance on the *k* folds is best.

Programming all this is laborious. Fortunately, the team behind scikit-learn have a whole <a href=http://scikit-learn.org/stable/modules/cross_validation.html>cross-validation module</a>. In addition, the library also has a class specifically designed to perform grid search on the parameters you specify, which is called <a href=http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html>GridSearchCV</a>:


```python
from sklearn.grid_search import GridSearchCV
```

As the documentation states, in order to build an instance of *GridSearchCV*, you will need to specify two objects:
* An **estimator**, that is, the internal model that is optimized (in this case an *SVC* instance).
* A **parameter grid**, a dictionary where you specify the parameter names and the ranges to perform the search on (these ranges were defined above).

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Use the code cell below to create a *GridSearchCV* object called *gs* with the SVM as estimator and the range for *C* and *gamma* we defined above.
 </td></tr>
</table>


```python
from sklearn.grid_search import GridSearchCV

param_grid = {'C':Cs, 'gamma':gammas, 'kernel':['rbf']}

gs= GridSearchCV(SVC(),param_grid,cv=10)

```

GridSearchCV is implemented as well as a model, so its fit(X, y) method is the one that performs the grid search, doing cross-validation under the hood.

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Call the fit method on your *gs* object to perform the grid search on the training set. Do not worry if it takes some time to complete. 
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
If you build your *GridSearchCV* object above with *verbose=1* you will see a trace of how the cross-validation process is going. For more details, set *verbose=2*. If your PC has several processors you can speed up the whole process by setting *n_jobs* accordingly.
 </td></tr>
</table>


```python
gs.fit(X_train,y_train)
```




    GridSearchCV(cv=10, error_score='raise',
           estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False),
           fit_params={}, iid=True, n_jobs=1,
           param_grid={'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04]), 'gamma': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03,
           1.e+04]), 'kernel': ['rbf']},
           pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)



Now that the grid search is complete, let us recover the results obtained. To keep things as before, we would like to overwrite the following variables:
* *accs*: accurary matrix for the different (*C*, *gamma*) pairs.
* *best_acc*: best accuracy.
* *best_C*: best value for *C*.
* *best_gamma*: best value for *gamma*.
* *best_model*: fitted model with the best *C* and the best *gamma*.

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Recover these variables from the fitted *gs* object. Check the *attributes* section in the <a href=http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html>documentation</a>.
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
You do not need to retrain *best_model* from *best_C* and *best_gamma*, it is available directly in the *best\_estimator\_* attribute (notice the final underscore). Think also about how to get the accuracies in matrix form from *grid\_scores\_*. Numpy's *reshape* function may come in handy.
 </td></tr>
</table>


```python
accs=gs.param_grid
best_acc=gs.best_score_
best_C=gs.best_params_['C']
best_gamma=gs.best_params_['gamma']
best_model=gs.best_estimator_

```

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Replot the accuracies just retrieved. In what way is the resulting figure different from the previous one?
 </td></tr>
</table>


```python
accs_raw=gs.grid_scores_
accs2 = np.zeros((len(Cs), len(gammas)))
cont=0
for i, C in enumerate(Cs):
    # For each sigma
    for j, gamma in enumerate(gammas):
        accs2[i, j] = accs_raw[cont][1]
        cont=cont+1
print(accs2)
```

    [[0.508 0.508 0.508 0.508 0.508 0.508 0.508 0.508 0.508]
     [0.508 0.648 0.716 0.712 0.556 0.508 0.508 0.508 0.508]
     [0.492 0.72  0.74  0.732 0.692 0.676 0.668 0.532 0.508]
     [0.56  0.744 0.748 0.724 0.684 0.676 0.676 0.532 0.508]
     [0.68  0.752 0.756 0.68  0.7   0.672 0.676 0.532 0.508]
     [0.724 0.74  0.748 0.68  0.684 0.672 0.676 0.532 0.508]
     [0.756 0.748 0.728 0.696 0.676 0.672 0.676 0.532 0.508]]
    


```python
plt.imshow(accs2, interpolation='nearest', cmap=plt.cm.hot)
plt.xlabel('gamma')
plt.ylabel('C')
plt.colorbar()
plt.xticks(np.arange(len(gammas)), gammas, rotation=45)
plt.yticks(np.arange(len(Cs)), Cs)
plt.title('Accuracies obtained')
plt.show()
```


![png](output_48_0.png)


If we implement gridsearchCV in the model (with crossvalidation). We create a mesh with multiple parameters and we combine them each other’s to get the best model. We can see in this model that, when you implement gridsearchCV we can get eliminate the overfitting of the model above, where we didn't apply gridseearchCV.

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Reprint the best parameters and their accuracy on training. Explain the changes you observe.
 </td></tr>
</table>


```python
print("accurary matrix",accs)
print("Best model: " , best_model,'\n')
print("Best accuracy: ",best_acc)
print("Best C ",best_C)
print("Best ganma ",best_gamma)
```

    accurary matrix {'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04]), 'gamma': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03,
           1.e+04]), 'kernel': ['rbf']}
    Best model:  SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False) 
    
    Best accuracy:  0.756
    Best C  100.0
    Best ganma  0.01
    

With crossvalidation both variables has changed

C: from 1 to 100
gamma: from 100 to 0.1

and the accuracy has change (in trainning) from 1 to 0.756. 

In the first example we had overfitting and this one it´s a better approach

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Recalculate the accuracy on test. Dit it improve?
 </td></tr>
</table>


```python
best_model = gs.best_estimator_
best_model.fit(X_train,y_train)

print (best_model.score(X_test, y_test))
```

    0.732
    

Yes, it improved from 0.628 to 0.732

## Pipelining

So far we have assumed that, once the data are loaded, they are ready to be fed to our model without any changes. If you inspect the code above, there is no change in the dataset since creation till it is fed to the *fit(X, y)* method (note that splitting the patterns in validation folds does not change the patterns themselves).

In practice this will rarely be the case. The data usually need some kind of previous transformations, such as making them all be uniform. This is what is known as **preprocessing**. Combining these transformations with other processes like cross-validation is tricky, as one must make sure that the transformations take place in each of the validation folds, so that the models are only fitted with properly transformed data.

Once more, we are lucky that scikit-learn makes this task very easy. The class that encapsulates all this is called <a href=http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html>Pipeline</a>:


```python
from sklearn.pipeline import Pipeline
```

Conceptually, a pipeline is defined as a series of **transformers**, followed by one final step which must be an estimator. In the scikit-learn's jargon, this means that the class that takes the last step must implement the method *fit(X, y)*, whereas all the previous classes must implement a new method called *transform(X)* that makes some modification on the input data *X*. If those transformations are data-dependent and have to be learnt, they must also implement another method called *fit_transform(X, y)*.

Scikit-learn hides this complexity by implementing Pipeline itself as a model as well. Thus, this class complies with the interface we saw in the previous section: 
* **fit(X, y)**: calls *fit_transform(X, y)* for all transformers, then calls *fit(X, y)* for the final estimator. This means that all transformations are successively learnt and applied before fitting the model with the properly transformed data.
* **score(X, y)**: calls *transform(X)* for all transformers, then calls *score(X, y)* for the final estimator. The final model requires transformed data, and all the transformations have been learnt by *fit(X, y)*, so they can be now applied directly.

As the <a href=http://scikit-learn.org/stable/modules/pipeline.html>usage guide</a> describes, in order to build an instance of <a href=http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html>Pipeline</a> you will need to specify a list of **steps**, that is, the transformers and the final estimator you wish to chain. For internal reference, each of these steps must be identified by a name as well as by its class, so the steps are passed as a list of (name, class) pairs. Consider the following example:


```python
Pipeline([('svm', SVC())])
```




    Pipeline(memory=None,
         steps=[('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False))])



Which builds a pipeline with just one estimator (the default SVM), named 'svm'. 

Obviously, a pipeline makes sense only when there is more than a single step. To keep things simple, here we will add just one previous step before the SVM, which performs **normalization to zero mean and unit variance**. This is carried out by the class <a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html>StandardScaler</a>:


```python
from sklearn.preprocessing import StandardScaler
```

This normalization makes all features be in the same scale, preventing that some feature may have a bigger influence than others just because of the different original scales. If you go back to the dataset's plot, you can see that the scale of the first coordinate is much larger than the second one ([-40, +60] range compared to [-0.3, +0.6]). If we applied an RBF SVM to the data as they are, when computing the kernel function the second coordinate would be negligible compared to the first one. 

However, the figure makes clear that both features are equally important for correct classification, so normalization should improve accuracies considerably. Let us see if this is indeed the case.

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Build a pipeline called *pl* with a *StandardScaler* followed by an SVM. Name both steps as you please.
 </td></tr>
</table>


```python
pl= Pipeline([('scaler', StandardScaler()), ('svm',SVC())])
pl.fit(X_train, y_train)
pl_score = pl.score(X_train, y_train)
pl_score
```




    0.872



Performing grid search is trivial now, since *pl* is a valid estimator for the *GridSearchCV* object.

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Rewrite the grid search for the pipeline. This should overwrite again the variables *accs*, *best_acc*, *best_C*, *best_gamma* and *best_model*.
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
Since the pipeline steps are named, the way to access parameters now is 'step\_\_parameter' (notice the double underscore). For example, in the pipeline above, we would access the *C* parameter with 'svm\_\_C'.
 </td></tr>
</table>


```python
pl= Pipeline([('scaler', StandardScaler()), ('svm',SVC())])

param_grid = {'svm__C':Cs, 'svm__gamma':gammas}

gs_pipe= GridSearchCV(pl,param_grid,cv=10)
gs_pipe.fit(X_train, y_train)


accs=gs_pipe.grid_scores_
best_acc=gs_pipe.best_score_
best_C=gs_pipe.best_params_['svm__C']
best_gamma=gs_pipe.best_params_['svm__gamma']
best_model=gs_pipe.best_estimator_

```

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Replot the new accuracies. Are they different from the ones without the scaler?
 </td></tr>
</table>


```python
accs_raw_pipe=gs_pipe.grid_scores_
accs3 = np.zeros((len(Cs), len(gammas)))
cont=0
for i, C in enumerate(Cs):
  # For each sigma
  for j, gamma in enumerate(gammas):
      accs3[i, j] = accs_raw_pipe[cont][1]
      cont=cont+1
       
plt.imshow(accs3, interpolation='nearest', cmap=plt.cm.hot)
plt.xlabel('gamma')
plt.ylabel('C')
plt.colorbar()
plt.xticks(np.arange(len(gammas)), gammas, rotation=45)
plt.yticks(np.arange(len(Cs)), Cs)
plt.title('Accuracies obtained')
plt.show()
```


![png](output_71_0.png)


Yes without scaler we got accuracies close to 0.75 and with Scaler the accuracies reach 0.90. Therefore normalization with StandardScaler improves the model.


<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Reprint the best parameters and their accuracy both on training and test. How did they change? Are they better now?
 </td></tr>
</table>


```python
print("Best model: " , best_model,'\n')
print("Best accuracy: ",best_acc)
print("Best C ",best_C)
print("Best ganma ",best_gamma)
print("accurary matrix",accs)

Pipe_Grid_estimator_SVC = gs_pipe.best_estimator_
Pipe_Grid_estimator_SVC.fit(X_train,y_train)

print("***********************************************************")
print("score train",Pipe_Grid_estimator_SVC.score(X_train, y_train))
print("score test",Pipe_Grid_estimator_SVC.score(X_test, y_test))
print("***********************************************************")
```

    Best model:  Pipeline(memory=None,
         steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm', SVC(C=1000.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma=1.0, kernel='rbf',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False))]) 
    
    Best accuracy:  0.916
    Best C  1000.0
    Best ganma  1.0
    accurary matrix [mean: 0.50800, std: 0.00980, params: {'svm__C': 0.01, 'svm__gamma': 0.0001}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.01, 'svm__gamma': 0.001}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.01, 'svm__gamma': 0.01}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.01, 'svm__gamma': 0.1}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.01, 'svm__gamma': 1.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.01, 'svm__gamma': 10.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.01, 'svm__gamma': 100.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.01, 'svm__gamma': 1000.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.01, 'svm__gamma': 10000.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.1, 'svm__gamma': 0.0001}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.1, 'svm__gamma': 0.001}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.1, 'svm__gamma': 0.01}, mean: 0.50400, std: 0.04381, params: {'svm__C': 0.1, 'svm__gamma': 0.1}, mean: 0.74400, std: 0.09419, params: {'svm__C': 0.1, 'svm__gamma': 1.0}, mean: 0.78400, std: 0.10638, params: {'svm__C': 0.1, 'svm__gamma': 10.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.1, 'svm__gamma': 100.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.1, 'svm__gamma': 1000.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 0.1, 'svm__gamma': 10000.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 1.0, 'svm__gamma': 0.0001}, mean: 0.50800, std: 0.00980, params: {'svm__C': 1.0, 'svm__gamma': 0.001}, mean: 0.50800, std: 0.00980, params: {'svm__C': 1.0, 'svm__gamma': 0.01}, mean: 0.65600, std: 0.08618, params: {'svm__C': 1.0, 'svm__gamma': 0.1}, mean: 0.88800, std: 0.05012, params: {'svm__C': 1.0, 'svm__gamma': 1.0}, mean: 0.88000, std: 0.03563, params: {'svm__C': 1.0, 'svm__gamma': 10.0}, mean: 0.78800, std: 0.08555, params: {'svm__C': 1.0, 'svm__gamma': 100.0}, mean: 0.61600, std: 0.09150, params: {'svm__C': 1.0, 'svm__gamma': 1000.0}, mean: 0.54000, std: 0.03970, params: {'svm__C': 1.0, 'svm__gamma': 10000.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 10.0, 'svm__gamma': 0.0001}, mean: 0.50800, std: 0.00980, params: {'svm__C': 10.0, 'svm__gamma': 0.001}, mean: 0.50400, std: 0.05640, params: {'svm__C': 10.0, 'svm__gamma': 0.01}, mean: 0.78800, std: 0.08546, params: {'svm__C': 10.0, 'svm__gamma': 0.1}, mean: 0.87200, std: 0.05554, params: {'svm__C': 10.0, 'svm__gamma': 1.0}, mean: 0.88400, std: 0.05467, params: {'svm__C': 10.0, 'svm__gamma': 10.0}, mean: 0.79200, std: 0.09519, params: {'svm__C': 10.0, 'svm__gamma': 100.0}, mean: 0.63600, std: 0.10670, params: {'svm__C': 10.0, 'svm__gamma': 1000.0}, mean: 0.53600, std: 0.04541, params: {'svm__C': 10.0, 'svm__gamma': 10000.0}, mean: 0.50800, std: 0.00980, params: {'svm__C': 100.0, 'svm__gamma': 0.0001}, mean: 0.49200, std: 0.05384, params: {'svm__C': 100.0, 'svm__gamma': 0.001}, mean: 0.61600, std: 0.07340, params: {'svm__C': 100.0, 'svm__gamma': 0.01}, mean: 0.84000, std: 0.09057, params: {'svm__C': 100.0, 'svm__gamma': 0.1}, mean: 0.88800, std: 0.04109, params: {'svm__C': 100.0, 'svm__gamma': 1.0}, mean: 0.88000, std: 0.05666, params: {'svm__C': 100.0, 'svm__gamma': 10.0}, mean: 0.79200, std: 0.09519, params: {'svm__C': 100.0, 'svm__gamma': 100.0}, mean: 0.63600, std: 0.10670, params: {'svm__C': 100.0, 'svm__gamma': 1000.0}, mean: 0.53600, std: 0.04541, params: {'svm__C': 100.0, 'svm__gamma': 10000.0}, mean: 0.48000, std: 0.03711, params: {'svm__C': 1000.0, 'svm__gamma': 0.0001}, mean: 0.51200, std: 0.04577, params: {'svm__C': 1000.0, 'svm__gamma': 0.001}, mean: 0.64800, std: 0.09604, params: {'svm__C': 1000.0, 'svm__gamma': 0.01}, mean: 0.84400, std: 0.05810, params: {'svm__C': 1000.0, 'svm__gamma': 0.1}, mean: 0.91600, std: 0.03493, params: {'svm__C': 1000.0, 'svm__gamma': 1.0}, mean: 0.86400, std: 0.07024, params: {'svm__C': 1000.0, 'svm__gamma': 10.0}, mean: 0.79200, std: 0.09519, params: {'svm__C': 1000.0, 'svm__gamma': 100.0}, mean: 0.63600, std: 0.10670, params: {'svm__C': 1000.0, 'svm__gamma': 1000.0}, mean: 0.53600, std: 0.04541, params: {'svm__C': 1000.0, 'svm__gamma': 10000.0}, mean: 0.51600, std: 0.04239, params: {'svm__C': 10000.0, 'svm__gamma': 0.0001}, mean: 0.61200, std: 0.07798, params: {'svm__C': 10000.0, 'svm__gamma': 0.001}, mean: 0.80000, std: 0.08298, params: {'svm__C': 10000.0, 'svm__gamma': 0.01}, mean: 0.86400, std: 0.05750, params: {'svm__C': 10000.0, 'svm__gamma': 0.1}, mean: 0.89200, std: 0.03857, params: {'svm__C': 10000.0, 'svm__gamma': 1.0}, mean: 0.86400, std: 0.07024, params: {'svm__C': 10000.0, 'svm__gamma': 10.0}, mean: 0.79200, std: 0.09519, params: {'svm__C': 10000.0, 'svm__gamma': 100.0}, mean: 0.63600, std: 0.10670, params: {'svm__C': 10000.0, 'svm__gamma': 1000.0}, mean: 0.53600, std: 0.04541, params: {'svm__C': 10000.0, 'svm__gamma': 10000.0}]
    ***********************************************************
    score train 0.98
    score test 0.892
    ***********************************************************
    

Without StandardScaler both variables has changed

C: from 100 to 1000
gamma: from 0.1 to 1.0

and the accuracy has change (in trainning) from 0.916. 

Many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.

and Yes, the model with the normalitation has improved

<table align="left">
 <tr><td width="80"><img src="img/pro.png" style="width:auto;height:auto"></td><td>
Accuracies do not tell you what the SVM is actually doing internally. Since the data are bidimensional, in this case we can plot what is going on. Write some code that calculates the SVM output (i.e., the distance to the hyperplane) for a mesh of points in the range of the inputs *X*. Your code should plot these distances, as well as the points *(X,y)* passed, using the color convention for the dataset: red for one class and blue for the other one. Explain with your own words the figure you obtain.
</td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
The distance to the hyperplane is computed by the SVC method **decision\_function(X)**, which is also part of the interface of scikit-learn's models. Your plot should look similar to the ones that appear in <a href=http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html>this example</a>. Feel free to base your code on the one appearing in the example.
 </td></tr>
</table>


```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

h = .02

clf =  best_model

figure = plt.figure(figsize=(15, 6))

# preprocess dataset, split into training and test part

X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = \
train_test_split(X, y, test_size=.4, random_state=42)

x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                 np.arange(y_min, y_max, h))

# just plot the dataset first
cm = plt.cm.RdBu
cm_bright = ListedColormap(['#FF0000', '#0000FF'])
ax = plt.subplot(1, 2, 1)
# Plot the training points
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
       edgecolors='k')
# and testing points
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,
       edgecolors='k')
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())

ax = plt.subplot(1, 2, 2)
clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
if hasattr(clf, "decision_function"):
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
else:
    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

# Put the result into a color plot
Z = Z.reshape(xx.shape)
ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)

# Plot also the training points
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
       edgecolors='k')
# and testing points
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
       edgecolors='k', alpha=0.6)

ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())

ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
    size=15, horizontalalignment='right')

plt.tight_layout()
plt.show()
```


![png](output_78_0.png)


## Training times

All the classes that scikit-learn provides for **non-linear SVMs** make use of **LIBSVM** internally. LIBSVM is regarded as the state-of-the-art piece of software for non-linear SVM training. It is based on the classical **SMO** algorithm.

SVMs are robust and have a very sound mathematical foundation. However, their main drawback in practical use is that, even if we use well-devised software to train them, the complexity is at least quadratic in the number of samples *n*. In big O notation, this is stated as **0(n<sup>2</sup>)**. This precludes the use of non-linear SVMs for very large datasets.

To illustrate this time growth, we will work with the *adult* dataset, available at the <a href=https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/>LIBSVM website</a>. There are 9 versions of this dataset with increasing training test sizes, ranging from roughly 1600 in the first one to more than 32000 in the last one. They have been downloaded for you in the *data* folder, under the names *adultX.svm*, where X ranges from 1 to 9.

Let us load for now the smallest version (*adult1.svm*). These .svm files have a special format to save space in disk. Fortunately, scikit-learn has some special functions to load this kind of files:


```python
from sklearn.datasets import load_svmlight_file, load_svmlight_files
```

As their name makes clear, the first function loads a single file, whereas the second one can load several ones. In the particular case of *adult1.svm*, we want to load also the test set *adult1_test.svm* for parameter tuning, so we can write:


```python
X_train, y_train, X_test, y_test = load_svmlight_files(("./data/adult1.svm", "./data/adult1_test.svm"))
print(X_train.shape)
print(X_test.shape)
```

    (1605, 123)
    (30956, 123)
    

As you can see, this dataset has 123 features. Grid search could take long, so here we will use **linear SVMs**. Recall that the default kernel function is RBF, so this has to be specified during construction of the SVC class:


```python
SVC(kernel='linear')
```




    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)



Thus, the *gamma* parameter will no longer be applicable, and the only parameter to tune is *C*. Let us use a narrower range now, as training tends to take a long time for large *C*: 


```python
Cs = np.logspace(-3, 2, 6)
```


```python
Cs
```




    array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])



<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Use the code of the previous section to tune a linear SVM on the *adult1* dataset.
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
A pipeline with normalization is not necessary in this case, since all features of the dataset turn out to be binary and scales are not an issue. Thus, the estimator inside *GridSearchCV* should be just a linear SVM. 
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
Since there is a single parameter *C* to optimize, *accs* should become now a vector instead of a matrix.
 </td></tr>
</table>


```python
param_grid = {'C':Cs, 'kernel':['linear']}
gs_linear= GridSearchCV(SVC(),param_grid,cv=10)
gs_linear.fit(X_train,y_train)
```




    GridSearchCV(cv=10, error_score='raise',
           estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False),
           fit_params={}, iid=True, n_jobs=1,
           param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]), 'kernel': ['linear']},
           pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)



<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Print the best *C* and its accuracy both on training and test. Verify that overfitting is not happening.
 </td></tr>
</table>


```python
acc_linear=gs_linear.grid_scores_
best_acc=gs_linear.best_score_
best_C=gs_linear.best_params_['C']

print("Best model: " , best_model,'\n')
print("Best accuracy: ",best_acc)
print("Best C ",best_C)
print("accurary vector",acc_linear)

best_model_linear = gs_linear.best_estimator_
best_model_linear.fit(X_train,y_train)

print (best_model_linear.score(X_train, y_train))
print (best_model_linear.score(X_test, y_test))
```

    Best model:  Pipeline(memory=None,
         steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm', SVC(C=1000.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma=1.0, kernel='rbf',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False))]) 
    
    Best accuracy:  0.8361370716510903
    Best C  0.1
    accurary vector [mean: 0.75389, std: 0.00235, params: {'C': 0.001, 'kernel': 'linear'}, mean: 0.82056, std: 0.02190, params: {'C': 0.01, 'kernel': 'linear'}, mean: 0.83614, std: 0.02861, params: {'C': 0.1, 'kernel': 'linear'}, mean: 0.83115, std: 0.03321, params: {'C': 1.0, 'kernel': 'linear'}, mean: 0.83053, std: 0.02951, params: {'C': 10.0, 'kernel': 'linear'}, mean: 0.82991, std: 0.02978, params: {'C': 100.0, 'kernel': 'linear'}]
    0.8467289719626169
    0.8430998837059052
    

We will use this value of *C* for all versions of the *adult* dataset. In order to measure execution times, Python has the native function *time*:


```python
from time import time
```

To measure how many seconds it takes to execute some code, place that code between two calls to *time* and measure the difference:


```python
t_start = time()
for i in range(1000000):
    pass    # do nothing
t_end = time()
t_end - t_start
```




    0.044973134994506836



One of the improvements of LIBSVM with respect to SMO is its **caching** strategy. For most real-life datasets, there are patterns that are ignored by LIBSVM since they do not influence the solution, whereas there are other patterns that are repeatedly optimized upon in several iterations. If we store in a cache the rows of the kernel matrix corresponding to these repeated patterns, we can save the time needed to recalculate them from scratch.

Obviously, the larger the cache is, the more rows it is able to store and the bigger its time-saving potential is. This size is controlled by the *cache_size* parameter of *SVC*, which defaults to 200 MB unless specified otherwise on construction:


```python
best_model=best_model_linear
best_model
```




    SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)



Let us try with these sizes:


```python
caches = [100, 200, 500]
```

Our goal is to plot training time versus number of patterns, repeating the process for these 3 cache sizes and the increasingly large *adult* versions. There are 9 of these, but we will work with just the first 6:


```python
num_adults = 6
```

For example, for the default cache of 200 MB, the training time of the smallest *adult* version (the one currently loaded) is:


```python
t_start = time()
best_model.fit(X_train, y_train)
t_end = time()
t = t_end - t_start
print((len(y_train), t))
```

    (1605, 0.1009373664855957)
    

But this is is just a single point of the plot we want. Your task is to compute the training times for all combinations of cache sizes and *adult* versions, storing them in the following matrix:


```python
times = np.zeros((num_adults, len(caches)))
times
```




    array([[0., 0., 0.],
           [0., 0., 0.],
           [0., 0., 0.],
           [0., 0., 0.],
           [0., 0., 0.],
           [0., 0., 0.]])



Whereas the dataset sizes will be stored in the following vector:


```python
sizes = np.zeros((num_adults, 1))
sizes
```




    array([[0.],
           [0.],
           [0.],
           [0.],
           [0.],
           [0.]])



<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Write the code that fills *times* and *sizes*.
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
In order to minimize disk accesses, load each version of the *adult* dataset in an outer loop, and do the training for the different cache sizes in an inner loop.
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
Test sets are not needed, so it is enough to use *load_svmlight_file* to load the training sets.
 </td></tr>
</table>


```python
files=["adult1.svm","adult2.svm","adult3.svm","adult4.svm","adult5.svm","adult6.svm"]

i=0
for file in files:
    cad="./data/"+ file
    X_train, y_train = load_svmlight_file((cad))
    sizes[i,]=X_train.shape[0]
    
    j=0
    for cache in caches:
        t_start = time()
        
        param_grid = {'C':Cs, 'kernel':['linear'], 'cache_size':[cache]}
        gs_linear= GridSearchCV(SVC(),param_grid,cv=10)
        gs_linear.fit(X_train,y_train)
        
        t_end = time()
        tiempo= t_end - t_start
        print(tiempo)
        times[i,j]=tiempo
        j=j+1   
    i=i+1
```

    28.787455320358276
    28.251641988754272
    29.14637565612793
    50.94729399681091
    49.17236590385437
    49.19836187362671
    81.21171855926514
    80.82842469215393
    80.72213172912598
    157.2742257118225
    157.3619613647461
    157.55839705467224
    259.3112139701843
    267.422483921051
    282.5125904083252
    1024.092652797699
    750.6125648021698
    721.2697710990906
    


```python
times
```




    array([[  28.78745532,   28.25164199,   29.14637566],
           [  50.947294  ,   49.1723659 ,   49.19836187],
           [  81.21171856,   80.82842469,   80.72213173],
           [ 157.27422571,  157.36196136,  157.55839705],
           [ 259.31121397,  267.42248392,  282.51259041],
           [1024.0926528 ,  750.6125648 ,  721.2697711 ]])




```python
sizes
```




    array([[ 1605.],
           [ 2265.],
           [ 3185.],
           [ 4781.],
           [ 6414.],
           [11220.]])




At this point we have all the necessary information for plotting.

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Use the code below to plots *times* versus *sizes*. What trend do you observe in the figure? Does this comply with the theoretical complexity? From what point does cache size begin to be relevant?
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
LIBSVM's complexity ranges from O(n<sup>2</sup>) to O(n<sup>3</sup>), depending on factors such as cache efficiency and the dataset itself. For example, the more features the dataset has, the longer training takes, since the complexity of computing the kernel function scales linearly with the number of features.
 </td></tr>
</table>


```python
for i, c in enumerate(caches):
    plt.plot(sizes, times[:, i], label='Cache ' + str(c) + ' MB')
plt.xlabel('Number of patterns')
plt.ylabel('Training time (seconds)')
plt.legend(loc='best')
plt.title('Training times')
plt.show()
```


![png](output_120_0.png)


We can see a parabola trend. you are aware of up sizes increase quickly the time. In all datasets the features numbers are equal. The only thing changes are the size of the dataset.

The cache size begins to be relevant with 200 MB and a number of patterns bigger than 6000. And the other hand with cache size higher 200 MB doesn't improve the time.

<table align="left">
 <tr><td width="80"><img src="img/pro.png" style="width:auto;height:auto"></td><td>
Complete your study with the largest versions (*adult7*, *adult8* and *adult9*) and an additional cache size of 1000 MB. Replot the results. Is the previous trend confirmed?
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
Because of time complexity, it may well be the case that these large datasets take a long time to train (minutes or even hours, depending on your computer). Use traces in your code to make sure everything is going fine.
 </td></tr>
</table>

#### add column with cache_size=1000


```python
times2 = np.zeros((num_adults, 1))    


i=0
for file in files:
    cad="./data/"+ file
    X_train, y_train = load_svmlight_file((cad))
    t_start = time()
    
    param_grid = {'C':Cs, 'kernel':['linear'], 'cache_size':[1000]}
    gs_linear= GridSearchCV(SVC(),param_grid,cv=10)
    gs_linear.fit(X_train,y_train)
    t_end = time()
    tiempo= t_end - t_start
    print(tiempo)
    times2[i,0]=tiempo

    i=i+1

times_exp = np.append(times, times2, axis=1)
times_exp
```

    29.751295804977417
    51.87636661529541
    84.99938941001892
    164.17631840705872
    282.30806374549866
    781.6983644962311
    




    array([[  28.78745532,   28.25164199,   29.14637566,   29.7512958 ],
           [  50.947294  ,   49.1723659 ,   49.19836187,   51.87636662],
           [  81.21171856,   80.82842469,   80.72213173,   84.99938941],
           [ 157.27422571,  157.36196136,  157.55839705,  164.17631841],
           [ 259.31121397,  267.42248392,  282.51259041,  282.30806375],
           [1024.0926528 ,  750.6125648 ,  721.2697711 ,  781.6983645 ]])



#### add numpy array with the 3 new files


```python
files2=["adult7.svm","adult8.svm","adult9.svm"]
caches = [100, 200, 500, 1000]
 
sizes2 = np.zeros((len(files2), 1))
times3 = np.zeros((len(files2), len(caches)))

i=0
for file in files2:
    cad="./data/"+ file
    X_train, y_train = load_svmlight_file((cad))
    sizes2[i,]=X_train.shape[0]
    
    j=0
    for cache in caches:
        t_start = time()
       
        param_grid = {'C':Cs, 'kernel':['linear'], 'cache_size':[cache]}
        gs_linear= GridSearchCV(SVC(),param_grid,cv=10,verbose=1,n_jobs=-1)
        gs_linear.fit(X_train,y_train)

        t_end = time()
        tiempo= t_end - t_start
        print(tiempo)
        times3[i,j]=tiempo
        j=j+1   
    i=i+1
    
times_exp2 = np.append(times_exp, times3, axis=0)
sizes_exp= np.append(sizes, sizes2, axis=0)
times_exp2
```

    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.1min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 16.6min finished
    

    1002.2921733856201
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.0min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 10.4min finished
    

    630.9795386791229
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.2min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  7.7min finished
    

    469.81660890579224
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.3min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  7.8min finished
    

    479.3279812335968
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.1min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 43.5min finished
    

    2637.7905061244965
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.0min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 29.9min finished
    

    1823.2640883922577
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.6min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 17.8min finished
    

    1092.0047543048859
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.8min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 15.6min finished
    

    963.6861624717712
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  8.4min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 103.1min finished
    

    6254.410810470581
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  8.2min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 77.6min finished
    

    4721.464632987976
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  8.8min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 48.9min finished
    

    3002.916261911392
    Fitting 10 folds for each of 6 candidates, totalling 60 fits
    

    [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 10.2min
    [Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 35.4min finished
    

    2190.4270238876343
    




    array([[  28.78745532,   28.25164199,   29.14637566,   29.7512958 ],
           [  50.947294  ,   49.1723659 ,   49.19836187,   51.87636662],
           [  81.21171856,   80.82842469,   80.72213173,   84.99938941],
           [ 157.27422571,  157.36196136,  157.55839705,  164.17631841],
           [ 259.31121397,  267.42248392,  282.51259041,  282.30806375],
           [1024.0926528 ,  750.6125648 ,  721.2697711 ,  781.6983645 ],
           [1002.29217339,  630.97953868,  469.81660891,  479.32798123],
           [2637.79050612, 1823.26408839, 1092.0047543 ,  963.68616247],
           [6254.41081047, 4721.46463299, 3002.91626191, 2190.42702389]])




```python
sizes_exp
```




    array([[ 1605.],
           [ 2265.],
           [ 3185.],
           [ 4781.],
           [ 6414.],
           [11220.],
           [16100.],
           [22696.],
           [32561.]])




```python
for i, c in enumerate(caches):
    plt.plot(sizes_exp, times_exp2[:, i], label='Cache ' + str(c) + ' MB')
plt.xlabel('Number of patterns')
plt.ylabel('Training time (seconds)')
plt.legend(loc='best')
plt.title('Training times')
plt.show()
```


![png](output_129_0.png)


This time growth always happens with non-linear SVMs, even if we use the best algorithms available. Note however that we have been experimenting with a linear SVM. For linear SVMs there are more efficient solvers for large datasets, based on techniques like coordinate gradient descent (LIBLINEAR) or stochastic gradient descent (Pegasos). Scikit-learn knows about this, and provides a specific implementation for a linear SVM that invokes LIBLINEAR called <a href=http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html>LinearSVC</a>:


```python
from sklearn.svm import LinearSVC
LinearSVC()
```




    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
         verbose=0)



Note that there is no cache parameter now, since LIBLINEAR does not make use of a cache. On the other hand, LIBLINEAR is a flexible algorithm that can be used to train other models different than SVMs. To make it train an actual SVM, it must be constructed with *loss='hinge'*. Besides, since it has a random component, we will make use of *RND_STATE*. The rest of parameters are fine with their defaults:


```python
LinearSVC(loss='hinge', random_state=RND_STATE)
```




    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
         penalty='l2',
         random_state=<mtrand.RandomState object at 0x000001E1722ADF30>,
         tol=0.0001, verbose=0)



Since this class takes a lot less time to train, we can now use all 9 versions of the *adult* dataset. As there is no cache now, we should reshape *times* to vector form:


```python
num_adults = 9
times = np.zeros((num_adults, 1))
sizes = np.zeros((num_adults, 1))
```

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Rewrite the code that fills *times* and *sizes*, using now a *LinearSVC* instance instead of an *SVC* one.
 </td></tr>
</table>


```python
from sklearn.svm import LinearSVC

Cs = np.logspace(-3, 2, 6)
#caches = [100, 200, 500]
RND_STATE=100

num_adults = 9
times = np.zeros((num_adults, 1))
sizes = np.zeros((num_adults, 1))

files=["adult1.svm","adult2.svm","adult3.svm","adult4.svm","adult5.svm","adult6.svm","adult7.svm","adult8.svm","adult9.svm"]

i=0
j=0
for file in files:
    cad="./data/"+ file
    X_train, y_train = load_svmlight_file((cad))
    print(i)
    sizes[i,]=X_train.shape[0]
    
    
    t_start = time()
    
    param_grid = {'C':Cs,'loss':['hinge'],'random_state':[RND_STATE]}
    gs_linear= GridSearchCV(LinearSVC(),param_grid,cv=10)
    gs_linear.fit(X_train,y_train)

    t_end = time()
    tiempo= t_end - t_start
    print(tiempo)
    times[i,j]=tiempo
  
    i=i+1
```

    0
    1.4321129322052002
    1
    2.1530802249908447
    2
    2.6612613201141357
    3
    4.423371315002441
    4
    6.1671462059021
    5
    11.519582748413086
    6
    17.01430368423462
    7
    28.367400884628296
    8
    54.55615472793579
    

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Plot the times obtained versus the training set sizes. Compare this figure with the one you got for the *SVC* class. What is the time complexity for *LinearSVC*?
 </td></tr>
</table>


```python
plt.plot(sizes, times[:, 0], label='LinearSCV')
plt.xlabel('Number of patterns')
plt.ylabel('Training time (seconds)')
plt.legend(loc='best')
plt.title('Training times')
plt.show()
```


![png](output_139_0.png)


We can see that LinearSVC model is faster respect model above. we could think that it is due to this is a model with less complex respect, other models.

## Custom kernels

To conclude this assignment, we will deal with custom kernels for non-numerical data. 

As we saw in the pipelining section, scikit-learn implements several transformers that help in adapting data to the specific needs of the different estimators. However, these are usually numerical transformations. What if the data we want to work with are non-numerical, such as images or texts? Good news: scikit-learn also provides transformer classes for these kinds of data. 

In this exercise, we will concentrate on texts. Texts are problematic not only because they are non-numeric, but also because they typically have different lengths, punctuation marks, uppercase and lowercase letters, etcetera. The most common strategy in <a href=http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction>text preprocessing</a> is to transform a series of texts (**corpus**) into a **bag-of-words** representation. In plain language, this means:

* Standardizing all texts in the corpus into a common form (e.g., lowercase with punctuation removed).
* Splitting them in different units called **tokens** (e.g., words).
* Counting how many times each token appears.

Thus, each text will be transformed into a vector of length *d*, where *d* is the number of different tokens in the corpus. Its i-th element will be the number of times the i-th token appears in the text. This transformation is carried out by the <a href=http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html>CountVectorizer</a> class:


```python
from sklearn.feature_extraction.text import CountVectorizer
CountVectorizer()
```




    CountVectorizer(analyzer='word', binary=False, decode_error='strict',
            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
            lowercase=True, max_df=1.0, max_features=None, min_df=1,
            ngram_range=(1, 1), preprocessor=None, stop_words=None,
            strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
            tokenizer=None, vocabulary=None)



By default we can see that it tokenizes texts into lowercase words. Let us illustrate how it works with a simple corpus with 3 short texts:


```python
corpus = ['This is the first text.', 'This is the second text.', 'And yet another text: the third text.']
```

Fitting this corpus splits the different words, which are available in the *vocabulary\_* attribute:


```python
cvec = CountVectorizer().fit(corpus)
cvec.vocabulary_
```




    {'this': 8,
     'is': 3,
     'the': 6,
     'first': 2,
     'text': 5,
     'second': 4,
     'and': 0,
     'yet': 9,
     'another': 1,
     'third': 7}



We can see that the i-th position of each word is determined by alphabetical order and that punctuation has been removed. The transformed texts will thus be:


```python
X = cvec.transform(corpus)
X
```




    <3x10 sparse matrix of type '<class 'numpy.int64'>'
    	with 16 stored elements in Compressed Sparse Row format>



They are stored in a **sparse** matrix X, which only stores the words present. For example, the third text (that with index 2) is:


```python
print(X[2,:])
```

      (0, 0)	1
      (0, 1)	1
      (0, 5)	2
      (0, 6)	1
      (0, 7)	1
      (0, 9)	1
    

That is, it has 2 occurrences of the 5-th word in the vocabulary ('text'), and 1 ocurrence of the 0-th, 1-st, 6-th, 7-th and 9-th words ('and', 'another', 'the', 'third' and 'yet'). Words not occurring have a count of 0 and they are not stored in *X* to save space. This sparse format may seem cumbersome for this little example, but it is very compact for extensive corpuses that yield large vocabularies. Usually each text (usually called **document**) will have only a few words from the whole vocabulary, so most of its features will be 0 and there is no need to store that. 

For example, the *20 newsgroups* dataset available in scikit-learn has 18000 documents. It can be loaded with the following utility function:


```python
from sklearn.datasets import fetch_20newsgroups
```

The documents are pieces of news belonging to 20 categories. Here we will only use 2 of them (atheism and religion) in order to cast a binary classification problem. The source file is available in the *data* folder. Let us load the training data, which has 857 documents (480 talking about atheism, 377 about religion):


```python
RND_STATE=150
```


```python
categories=['alt.atheism', 'talk.religion.misc']
data = fetch_20newsgroups(subset='train', categories=categories, data_home="./data", 
                          download_if_missing=False, random_state=RND_STATE)
X_train, y_train = data.data, data.target
print(data.target_names)
print(len(X_train))
print(sum(y_train == 0))
print(sum(y_train == 1))
```

    ['alt.atheism', 'talk.religion.misc']
    857
    480
    377
    

A document looks like this:


```python
X_train[0]
```




    'From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Rosenau)\nSubject: Re: Yeah, Right\nOrganization: Technical University Braunschweig, Germany\nLines: 54\n\nIn article <66014@mimsy.umd.edu>\nmangoe@cs.umd.edu (Charley Wingate) writes:\n \n>>And what about that revelation thing, Charley?\n>\n>If you\'re talking about this intellectual engagement of revelation, well,\n>it\'s obviously a risk one takes.\n>\n \nI see, it is not rational, but it is intellectual. Does madness qualify\nas intellectual engagement, too?\n \n \n>>Many people say that the concept of metaphysical and religious knowledge\n>>is contradictive.\n>\n>I\'m not an objectivist, so I\'m not particularly impressed with problems of\n>conceptualization.  The problem in this case is at least as bad as that of\n>trying to explain quantum mechanics and relativity in the terms of ordinary\n>experience.  One can get some rough understanding, but the language is, from\n>the perspective of ordinary phenomena, inconsistent, and from the\n>perspective of what\'s being described, rather inexact (to be charitable).\n>\n \nExactly why science uses mathematics. QM representation in natural language\nis not supposed to replace the elaborate representation in mathematical\nterminology. Nor is it supposed to be the truth, as opposed to the\nrepresentation of gods or religions in ordinary language. Admittedly,\nnot  every religion says so, but a fancy side effect of their inept\nrepresentations are the eternal hassles between religions.\n \nAnd QM allows for making experiments that will lead to results that will\nbe agreed upon as being similar. Show me something similar in religion.\n \n \n>An analogous situation (supposedly) obtains in metaphysics; the problem is\n>that the "better" descriptive language is not available.\n>\n \nWith the effect that the models presented are useless. And one can argue\nthat the other way around, namely that the only reason metaphysics still\nflourish is because it makes no statements that can be verified or falsified -\nshowing that it is bogus.\n \n \n>>And in case it holds reliable information, can you show how you establish\n>>that?\n>\n>This word "reliable" is essentially meaningless in the context-- unless you\n>can show how reliability can be determined.\n \nHaven\'t you read the many posts about what reliability is and how it can\nbe acheived respectively determined?\n   Benedikt\n'



Note all the punctuation marks, whitespaces, linebreaks... Fortunately, *CountVectorizer* will take care of all that internally and will transform *X_train* into a numerical matrix suitable for an SVM. There are 3 possibilities for this SVM:
* A linear SVM.
* A non-linear SVM with some existing kernel (e.g., RBF).
* A non-linear SVM with a custom kernel for this task.

We covered the first 2 kinds of SVMs in the previous sections, so let us consider the third one. The class *SVC* allows to specify a user-defined function for the *kernel* attribute. The <a href=http://scikit-learn.org/stable/modules/svm.html#svm-kernels>documentation</a> specifies that your custom kernel must take as arguments 2 matrices of shape (n1, d), (n2, d) and return a kernel matrix of shape (n1, n2).

Recall that a kernel measures similarity between patterns. Since our patterns will be word counts, we should do some operation that measures how similar two documents are, using their word-count vectors. A possibility that comes to mind is to count **how many words appear in both documents**. The following function calculates this:


```python
import numpy as np
def words_in_common(X1, X2):
    X1_bin = (X1 > 0).astype(np.int)     # binarize word counts in corpus X1: 1 = word appears, 0 = word does not appear
    X2_bin = (X2 > 0).astype(np.int)     # same for second corpus X2
    # Now, for two documents in binary vector form, the number of matching words is the sum of 1s in the same positions.
    # For example, [0, 1, 1, 0] and [1, 1, 1, 0] have two 1s in the same position (2nd and 3rd), so they have 2 words in common.
    # Observation: this is the same as the sum of the dot product of both vectors.
    # Thus, in matrix form we just have to transpose the second corpus, and call standard matrix multiplication.
    return X1_bin.dot(X2_bin.T)
```

Let us verify it works as expected:


```python
print(words_in_common(X, X))
```

      (0, 2)	2
      (0, 1)	4
      (0, 0)	5
      (1, 2)	2
      (1, 1)	5
      (1, 0)	4
      (2, 1)	2
      (2, 0)	2
      (2, 2)	6
    

Obviously, each document has all words in common with itself. The first one has 4 words in common with the second one (they only differ in the words 'first' and 'second'). The remaining cells are also correct, and the resulting matrix is symmetric, as every kernel matrix should be.

Now we are ready to try a pipeline with a *CountVectorizer* and an SVM with this custom kernel. We will use the same values for *C* than in the previous exercise.

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Build the pipeline and optimize it with grid search, as you did in the previous sections.
 </td></tr>
</table>


```python
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.datasets import fetch_20newsgroups
```


```python
Cs = np.logspace(-2, 4, 7)

pl= Pipeline([('vect', CountVectorizer()),('SVC', SVC(kernel=words_in_common))])
param_grid = {'SVC__C': Cs}

gs_pipe= GridSearchCV(pl,param_grid,cv=10,verbose=1)
gs_pipe.fit(X_train, y_train)

accs=gs_pipe.grid_scores_
best_acc=gs_pipe.best_score_
best_C=gs_pipe.best_params_['SVC__C']
best_model=gs_pipe.best_estimator_


print("Best model: " , best_model,'\n')
print("Best accuracy: ",best_acc)
print("Best C ",best_C)
print("accurary matrix",accs)
```

    Fitting 10 folds for each of 7 candidates, totalling 70 fits
    

    [Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:   43.1s finished
    

    Best model:  Pipeline(memory=None,
         steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
            lowercase=True, max_df=1.0, max_features=None, min_df=1,
            ngram_range=(1, 1), preprocessor=None, stop_words=None,
            strip..., max_iter=-1,
      probability=False, random_state=None, shrinking=True, tol=0.001,
      verbose=False))]) 
    
    Best accuracy:  0.9451575262543758
    Best C  0.1
    accurary matrix [mean: 0.93932, std: 0.02733, params: {'SVC__C': 0.01}, mean: 0.94516, std: 0.02381, params: {'SVC__C': 0.1}, mean: 0.94516, std: 0.02381, params: {'SVC__C': 1.0}, mean: 0.94516, std: 0.02381, params: {'SVC__C': 10.0}, mean: 0.94516, std: 0.02381, params: {'SVC__C': 100.0}, mean: 0.94516, std: 0.02381, params: {'SVC__C': 1000.0}, mean: 0.94516, std: 0.02381, params: {'SVC__C': 10000.0}]
    

    C:\Users\raul_\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
      DeprecationWarning)
    

<table align="left">
 <tr><td width="80"><img src="img/question.png" style="width:auto;height:auto"></td><td>
Report the value of *C* obtained, as well as the accuracies on training and test. How good are they?
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
You will need to load the test data. Load them like the training data.
 </td></tr>
</table>


```python
data2 = fetch_20newsgroups(subset='test', categories=categories, data_home="./data",
                      download_if_missing=False, random_state=0)
X_test, y_test = data2.data, data2.target

best_model.fit(X_train,y_train)

print (best_model.score(X_train, y_train))
print (best_model.score(X_test, y_test))
```

    1.0
    0.8017543859649123
    

The Best C is 0.1.
The Best Accuracy is 0.80 that is moderately good

<table align="left">
 <tr><td width="80"><img src="img/pro.png" style="width:auto;height:auto"></td><td>
One problem with the kernel *words_in_common* is that the information about how many times each word appears is lost. For example, if a given word W appears in a document D1 twice and in another document D2 just once, both documents are considered to match 100% for W. The same happens if that word appears twice both in D1' and D2'. In terms of similarity, though, it seems natural to consider that D1' and D2' are more similar for W than D1 and D2. A measure that takes this into account is the <a href=https://en.wikipedia.org/wiki/Cosine_similarity>cosine similarity</a>, defined as **u<sup>T</sup>v / (||u||*||v||)**, where **u** and **v** are the word-count vectors for both documents and **||·||** stands for the L2 norm. Define a function that implements cosine similarity, and tune the resulting pipeline. Can you obtain better results with this kernel?
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
Recall that ||u|| = sqrt(u<sup>T</sup>u). Thus, you can implement this kernel in terms of numpy's *dot* function. Scikit-learn's function <a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html>normalize</a> can also be helpful.
 </td></tr>
</table>


```python
from scipy.sparse import *
from scipy.sparse.linalg import norm

def cos_simi(X1, X2):
    a1=X1.A
    a2=X2.A
    a=np.dot(a1,a2.T)
    b1=np.linalg.norm(a1,2) # Second parameter 2-norm (largest sing. value)
    b2=np.linalg.norm(a2,2)
    b=b1 * b2    
    c=a/b 
    return c
Cs = np.logspace(-2, 4, 7)

pl= Pipeline([('vect', CountVectorizer()),('SVC', SVC(kernel=cos_simi))])
param_grid = {'SVC__C': Cs}

gs_pipe= GridSearchCV(pl,param_grid,cv=10,verbose=1)
gs_pipe.fit(X_train, y_train)

accs=gs_pipe.grid_scores_
best_acc=gs_pipe.best_score_
best_C=gs_pipe.best_params_['SVC__C']
best_model=gs_pipe.best_estimator_


print("Best model: " , best_model,'\n')
print("Best accuracy: ",best_acc)
print("Best C ",best_C)
print("accurary matrix",accs)


best_model.fit(X_train,y_train)

best_model_train=best_model.score(X_train, y_train)
best_model_test=best_model.score(X_test, y_test)
print ("SCV score train:",best_model_train)
print ("SVC score test:",best_model_test)
```

    Fitting 10 folds for each of 7 candidates, totalling 70 fits
    

    [Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed: 74.6min finished
    C:\Users\raul_\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
      DeprecationWarning)
    

    Best model:  Pipeline(memory=None,
         steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
            lowercase=True, max_df=1.0, max_features=None, min_df=1,
            ngram_range=(1, 1), preprocessor=None, stop_words=None,
            strip..., max_iter=-1,
      probability=False, random_state=None, shrinking=True, tol=0.001,
      verbose=False))]) 
    
    Best accuracy:  0.9241540256709452
    Best C  10000.0
    accurary matrix [mean: 0.56009, std: 0.00301, params: {'SVC__C': 0.01}, mean: 0.56009, std: 0.00301, params: {'SVC__C': 0.1}, mean: 0.56009, std: 0.00301, params: {'SVC__C': 1.0}, mean: 0.56009, std: 0.00888, params: {'SVC__C': 10.0}, mean: 0.63827, std: 0.04200, params: {'SVC__C': 100.0}, mean: 0.81914, std: 0.04712, params: {'SVC__C': 1000.0}, mean: 0.92415, std: 0.02617, params: {'SVC__C': 10000.0}]
    SCV score train: 0.9883313885647608
    SVC score test: 0.7807017543859649
    

No, the result is worst than the above: 0.78<0.80

<table align="left">
 <tr><td width="80"><img src="img/pro.png" style="width:auto;height:auto"></td><td>
Compare your results with the ones obtained by a linear SVM. Even if linear SVMs tend to give worse results, they have the advantage of the model's interpretability. Since the weight vector **w** does not depend on an unknown feature map, we can retrieve it after training. Moreover, because the decision function is **w·x + b** and one class is positive (y = +1) and the other is negative (y = -1), the components of **w** can be interpreted as the importance of the variables **x**: the more positive the i-th component of **w**, the more related is that variable to the positive class. Conversely, the more negative, the more related to the negative class. In this particular case, the features **x** are word counts, so the sign of **w** gives us how much a word indicates atheism (positive class) or religion (negative class). Once you have tuned your linear SVM, find the 10 most indicative words for each class. Do they make sense?
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
**w** is available in the *coef\_* attribute of the SVM. Since it is stored as a sparse matrix, you will need to transform it into a dense vector.
 </td></tr>
</table>

<table align="left">
 <tr><td width="80"><img src="img/exclamation.png" style="width:auto;height:auto"></td><td>
Once you retrieve **w**, you can find its largest and smallest components with numpy's *argsort* function. Note that the *CountVectorizer*'s *vocabulary_* attribute is in dictionary form, so you cannot find the words from their indices. Use instead the word list returned by the method *get_feature_names* of the *CountVectorizer* class.
</td></tr>
</table>


```python
from sklearn.svm import LinearSVC
```


```python
pl= Pipeline([('vect', CountVectorizer()),('SVC', LinearSVC())])

param_grid = {'SVC__C':Cs}

gs_pipe= GridSearchCV(pl,param_grid,cv=10,verbose=1)
gs_pipe.fit(X_train, y_train)

accs=gs_pipe.grid_scores_
best_acc=gs_pipe.best_score_
best_C=gs_pipe.best_params_['SVC__C']

best_model=gs_pipe.best_estimator_


print("Best model: " , best_model,'\n')
print("Best accuracy: ",best_acc)
print("Best C ",best_C)
print("accurary matrix",accs)


best_model_linear = best_model
best_model_linear.fit(X_train,y_train)

best_model_linear_train=best_model_linear.score(X_train, y_train)
best_model_linear_test=best_model_linear.score(X_test, y_test)
print ("SCV Linear score train:",best_model_linear_train)
print ("SVC Linear test:",best_model_linear_test)

print ("SCV score train:",best_model_train)
print ("SVC score test:",best_model_test)

```

    Fitting 10 folds for each of 7 candidates, totalling 70 fits
    

    [Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:   43.1s finished
    C:\Users\raul_\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
      DeprecationWarning)
    

    Best model:  Pipeline(memory=None,
         steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
            lowercase=True, max_df=1.0, max_features=None, min_df=1,
            ngram_range=(1, 1), preprocessor=None, stop_words=None,
            strip...ax_iter=1000,
         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
         verbose=0))]) 
    
    Best accuracy:  0.941656942823804
    Best C  0.01
    accurary matrix [mean: 0.94166, std: 0.01719, params: {'SVC__C': 0.01}, mean: 0.93932, std: 0.02184, params: {'SVC__C': 0.1}, mean: 0.93699, std: 0.02388, params: {'SVC__C': 1.0}, mean: 0.93699, std: 0.02388, params: {'SVC__C': 10.0}, mean: 0.93699, std: 0.02388, params: {'SVC__C': 100.0}, mean: 0.93699, std: 0.02388, params: {'SVC__C': 1000.0}, mean: 0.93699, std: 0.02388, params: {'SVC__C': 10000.0}]
    SCV Linear score train: 1.0
    SVC Linear test: 0.8035087719298246
    SCV score train: 0.9883313885647608
    SVC score test: 0.7807017543859649
    

these are the comparative results. The Linear SVC improve the SVC 

SVC Linear test: 0.8035087719298246

SVC score test: 0.7807017543859649


```python
best_model
```




    Pipeline(memory=None,
         steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
            lowercase=True, max_df=1.0, max_features=None, min_df=1,
            ngram_range=(1, 1), preprocessor=None, stop_words=None,
            strip...ax_iter=1000,
         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
         verbose=0))])




```python
my_model=gs_pipe.best_estimator_.named_steps['SVC']
my_model
```




    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
         verbose=0)




```python
m_coef=my_model.coef_
m_coef
```




    array([[ 0.00807405, -0.02179962,  0.        , ..., -0.00164719,
             0.00506788,  0.00506788]])




```python
corpus=X_train
cvec = CountVectorizer().fit(corpus)
vocabulary=cvec.get_feature_names()
```


```python
#taking the indices and sorting them
m_coef_atheism=np.argsort(m_coef)[0]
m_coef_religion=m_coef_atheism[::-1]

m_coef_atheism=m_coef_atheism[0:10]
m_coef_religion=m_coef_religion[0:10]
```


```python
print("10 most indicative words for ATHEISM")
for i in m_coef_atheism:
    print(vocabulary[i])
```

    10 most indicative words for ATHEISM
    atheists
    keith
    atheism
    free
    caltech
    okcforum
    cobb
    ibm
    wingate
    thing
    


```python
print("10 most indicative words for RELIGION")
for i in m_coef_religion:
    print(vocabulary[i])
```

    10 most indicative words for RELIGION
    christian
    buffalo
    mail
    virginia
    order
    org
    usa
    convenient
    mr
    organization
    

<center>
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>
                          THIS IS THE END OF THE ASSIGNMENT<br>
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>
</center>
